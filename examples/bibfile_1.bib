@article{Sampson2016,
author = {Sampson, Adrian},
pages = {1--3},
title = {{Statistical Mistakes and How to Avoid Them}},
year = {2016}
}
@article{Stone1974,
abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescrip- tion. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
author = {Stone, M},
doi = {10.2307/2984809},
file = {:Users/sebastian/Documents/Library/Mendeley/Stone/1974/Stone{\_}1974{\_}Cross-Validatory Choice and Assessment of Statistical Predictions.pdf:pdf},
isbn = {00359246},
issn = {00359246},
journal = {Journal of the Royal Statistical Society},
keywords = {CHOICE OF VARIABLES,CROSSVALIDATION,DOUBLECROSS,MODELMIX,MULTIPLE REGRESSION,PREDICTION,PRESCRIPTION,UNIVARIATE ESTIMATION,analysis of variance},
number = {2},
pages = {111--147},
pmid = {395},
title = {{Cross-Validatory Choice and Assessment of Statistical Predictions}},
volume = {36},
year = {1974}
}
@article{Cawley2010,
abstract = {Model selection strategies for machine learning algorithms typically involve$\backslash$nthe numerical optimisation of an appropriate model selection criterion, often$\backslash$nbased on an estimator of generalisation performance, such as k-fold $\backslash$ncross-validation.  The error of such an estimator can be broken down into bias $\backslash$nand variance components.  While unbiasedness is often cited as a beneficial $\backslash$nquality of a model selection criterion, we demonstrate that a low variance is $\backslash$nat least as important, as a non-negligible variance introduces the potential $\backslash$nfor over-fitting in model selection as well as in training the model.  While $\backslash$nthis observation is in hindsight perhaps rather obvious, the degradation in $\backslash$nperformance due to over-fitting the model selection criterion can be $\backslash$nsurprisingly large, an observation that appears to have received little $\backslash$nattention in the machine learning literature to date.  In this paper, we show $\backslash$nthat the effects of this form of over-fitting are often of comparable $\backslash$nmagnitude to differences in performance between learning algorithms, and thus $\backslash$ncannot be ignored in empirical evaluation.  Furthermore, we show that some $\backslash$ncommon performance evaluation practices are susceptible to a form of selection $\backslash$nbias as a result of this form of over-fitting and hence are unreliable.  We $\backslash$ndiscuss methods to avoid over-fitting in model selection and subsequent $\backslash$nselection bias in performance evaluation, which we hope will be incorporated $\backslash$ninto best practice.  While this study concentrates on cross-validation based $\backslash$nmodel selection, the findings are quite general and apply to any model $\backslash$nselection practice involving the optimisation of a model selection criterion $\backslash$nevaluated over a finite sample of data, including maximisation of the Bayesian $\backslash$nevidence and optimisation of performance bounds.},
author = {Cawley, Gavin C. and Talbot, Nicola L. C.},
file = {:Users/sebastian/Documents/Library/Mendeley/Cawley, Talbot/2010/Cawley, Talbot{\_}2010{\_}On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {bias-variance trade-off,model selection,over-,performance evaluation,selection bias},
pages = {2079−2107},
title = {{On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation}},
url = {http://jmlr.csail.mit.edu/papers/v11/cawley10a.html{\%}5Cnhttp://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf},
volume = {11},
year = {2010}
}
@article{Salzberg1997,
abstract = {An important component of many data mining projects is ﬁnding a good classiﬁcation algorithm, a process that requires very careful thought about experimental design. If not done very carefully, comparative studies of classiﬁcation and other types of algorithms can easily result in statistically invalid conclusions. This is especially true when one is using data mining techniques to analyze very large databases, which inevitably contain some statistically unlikely data. This paper describes several phenomena that can, if ignored, invalidate an experimental comparison. These phenomena and the conclusions that follow apply not only to classiﬁcation, but to computational experiments in almost any aspect of data mining. The paper also discusses why comparative analysis is more important in evaluating some types of algorithms than for others, and provides some suggestions about how to avoid the pitfalls suffered by many experimental studies.},
author = {Salzberg, Steven},
doi = {10.1023/A:1009752403260},
file = {:Users/sebastian/Documents/Library/Mendeley/Salzberg/1997/Salzberg{\_}1997{\_}On Comparing Classifiers Pitfalls to Avoid and a Recommended Approach.pdf:pdf},
isbn = {1384-5810},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {classification,comparative studies,statistical methods},
pages = {317--328},
title = {{On Comparing Classifiers : Pitfalls to Avoid and a Recommended Approach}},
volume = {328},
year = {1997}
}
